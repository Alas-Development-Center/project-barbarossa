# project/Barbarossa: Fine-tuning LLMs on Azerbaijani instruction-tuning dataset

![barbarossa](https://github.com/Alas-Development-Center/project-barbarossa/assets/31247506/c4b25e4d-06a3-4d84-ac5f-0ee0f9aa49b6)


1. **Introduction**

"project/Barbarossa", a pioneering initiative to fine-tune large language models (LLMs) on the Azerbaijani translation of the Stanford Alpaca dataset. This project represents a significant step in enhancing natural language processing capabilities for the Azerbaijani language. Utilizing the self-instruct method, our aim is to improve the understanding and generation of Azerbaijani text in AI models.

The project is named 'Barbarossa' inspired by the Ottoman naval commander Barbaros Hayrettin Pasha, a figure synonymous with exploration, mastery, and transformation in maritime history. Just as Barbarossa played a pivotal role in establishing Ottoman supremacy in the Mediterranean through strategic conquests and naval innovation, this project aims to navigate the uncharted waters of Azerbaijani language AI. The name 'Barbarossa' symbolizes our ambition to lead and innovate in this field, much like the historical figure who transformed the maritime dynamics of his era.


2. **Background**
   - Information about Stanford Alpaca dataset
   - The concept of instruction-tuning and self-instruct method
   - The need for Azerbaijani language support in LLMs

3. **Objectives**
   - Main goals of the project
   - Expected outcomes

4. **Methodology**
   - Detailed description of the fine-tuning process
   - Explanation of how the Stanford Alpaca dataset was translated into Azerbaijani
   - Information about the 30+ LLMs used

5. **Evaluation**
Accuracy Metrics: Present detailed statistics on the models' performance improvements, such as increased accuracy in language understanding and generation tasks when compared to baseline models. This could include metrics like BLEU scores for translation accuracy, F1 scores for question-answering tasks, and perplexity scores for language modeling.
   
Speed and Efficiency: Analyze the computational efficiency of the fine-tuned models in terms of processing speed and resource consumption. Comparisons with pre-fine-tuning performance can illustrate the optimizations achieved through the project.

Contextual Understanding: Through case studies or specific examples, demonstrate the models' improved understanding of context in Azerbaijani text. This might include their ability to discern subtle nuances in meaning and maintain context over longer stretches of text.

5. **Challenges and Limitations**
   
Technical Limitations: Discuss any technical hurdles encountered during the fine-tuning process, such as limitations in computing resources, difficulties in dataset translation, or challenges in model convergence.

Linguistic Limitations: Address linguistic challenges faced, including those related to translating the dataset and ensuring the models' nuanced understanding of Azerbaijani.

6. **Usage and Guidelines**
 We are excited to share our fine-tuned models with the community and encourage their use in a variety of applications. To ensure you can seamlessly integrate and utilize these models, please follow the instructions and take note of any prerequisites or dependencies detailed below:

  LLM name | LLM description | LLM paramater size | Pretrained LLM url (Huggingface) | 
--- | --- | --- | --- |
Seconds | 301 | 283 | 290 | 
      
7. **Team members**

The success of project/Barbarossa is a testament to the collective effort of a multidisciplinary team, comprising AI researchers, data scientists, linguists specializing in Azerbaijani language, software engineers, and project managers. Each member has contributed their expertise to navigate the challenges and achieve the project's objectives effectively. While individual contributions are numerous, the project's success is attributed to the cohesive and collaborative spirit of the entire team.


[Team Member Name]()

      
8. **Acknowledgement and Usage Terms**

"project/Barbarossa" is a proud product of the Alas Development Center (ADC). ADC, a leader in technological innovation and development, has spearheaded this project as part of its commitment to advancing the field of artificial intelligence, with a special focus on linguistic diversity and inclusion.

We are thrilled to offer these finely-tuned large language models to the public, free of charge. Our goal is to foster a collaborative and inclusive environment where technological advancements are accessible to all, especially for the development and enhancement of AI in the Azerbaijani language.


9. **Research and Development Notice**

__Please note that these models are provided primarily for research purposes. In our endeavor to contribute to the field, we have fine-tuned several large language models (LLMs) specifically for Azerbaijani language processing. Due to resource constraints, the fine-tuning process was limited to 5000 steps for some LLMs. This approach was adopted to demonstrate the potential and viability of enhancing Azerbaijani language capabilities within AI models, despite the limitations faced.__

__As such, while these models represent a significant step forward, they should be considered as part of ongoing research and development efforts. We encourage users to experiment with these models within their projects and research, bearing in mind the context of their training and the scope of their optimization.__

__By sharing these models, we hope to foster a collaborative and innovative environment where technological advancements are accessible to all, contributing to the diversification and inclusivity of AI research and development.__
